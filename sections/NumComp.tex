\section{Numerical Computation}
\begin{multicols}{2}
	\subsection{Overflow and Underflow}
	\textbf{Underflow} occurs when very small numbers are rounded to zero.
	Unfortunately, many functions behave for zero significantly different than for a small positive number, for example $\frac{1}{x}$ and $\log(x)$. In both cases, an overflow will result and further arithmetic will then not result in meaningful results.\\
	
	\textbf{Overflow} is less common. It occurs when the largest possible internal representation is reached.\\
	
	For example, the softmax function must be stabilized against under- and overflow:
	\begin{itemize}
		\item Assume that all $x_i$ have the same constant value $c$
		\item This implies that all classes represent the same probability of $1/n$
		\item Hence $1/n$ is the expected result of the softmax$(x)_i$ function, independent of $c$
		\item If the magnitude of $c$ is very large, this might not happen
		\begin{itemize}
			\item[$\rightarrow$] A very negative $c$ leads to $e^c$ to underflow, resulting in an $0/0$ problem
			\item[$\rightarrow$] A very positive $c$ leads to $e^c$ to overflow, resulting in an $\infty/\infty$ problem
		\end{itemize}
%	\columnbreak
		\item This is achieved by first subtracting the maximum entry of the vector $x$ from the other entries and then calculating the softmax function:
		$$\bol{z}=\bol{x}-\max_ix_i \quad\rightarrow\quad \text{softmax}(\bol{z})$$
		\item To stabilize against underflow which could happen with really small values, we use the log of the softmax:
		\[ \log \text{softmax}(\bol{x}) \]
	\end{itemize}
	
	\subsection{Gradient-Based Optimization}
	Learning requires a minimization of a loss function. The optimal value minimizing the loss function is often denoted with a superscript $^\ast$:
	\[ \bol{x}^\ast = \arg\,\min f(\bol{x}) \]
	
	The \textbf{derivative} specifies how a small input change $\epsilon$ results in the corresponding output change:
	\[ f(x+\epsilon) \approx f(x) + \epsilon f'(x) \]
	
	\textbf{Gradient descent} takes little steps $\epsilon (x\rightarrow x+\epsilon)$ such that $f(x+\epsilon)$ is smaller than $f(x)$.
	Clearly, if $f'(x)=0$, then the derivative provides no information in which direction the function could be minimized.\\
	
	Usually we need to \textbf{minimize loss functions} depending on a vector.
	Hence we have a scalar function $f(\bol{x})$ depending on many inputs:
	\[ f:\mathbb{R}^n\rightarrow \mathbb{R} \]
	
	In this case, the partial derivative is used to measure how $f()$ changes as all variables are held constant, except $x_i$ which increases at point $\bol{x}$. 
	The \textbf{gradient} generalizes the idea of a derivative in the sense that it represents a derivative of a scalar function with respect to a vector argument $\bol{x}$:
	\[ \nabla_x f(\bol{x}) =
	\begin{bmatrix} \frac{\partial f(\bol{x})}{\partial x_1}\\ \vdots \\ \frac{\partial f(\bol{x})}{\partial x_n} \end{bmatrix}  \]
	
	The method of \textbf{steepest descent} suggests a new point
	\[ \bol{x}' = \bol{x}-\epsilon\nabla_xf(\bol{x}) \]
	where $\epsilon$ is a small positive scalar and it's called the \textbf{learning rate}.
	
	\subsection{Jacobian and Hessian Matrices}
	If the function $\bol{f}(\bol{x})$ is a vector valued function of a vector argument $\bol{x}$, then the matrix of all possible partial derivatives is called the \textbf{Jacobian}:
	\[ \bol{f}:\mathbb{R}^m\rightarrow \mathbb{R}^n \qquad \bol{J}\in \mathbb{R}^{n\times m} \]
	
	Hence the entry $J_{i,j}$ is the partial derivative of the $i^{\text{th}}$ entry of $\bol{f}()$ with respect to the $j^{\text{th}}$ entry of $\bol{x}$:
	\[ J_{i,j} = \frac{\partial}{\partial x_j} f(\bol{x})_i \]
	
	The second derivative, also called the \textbf{curvature}, is denoted as
	\[ \frac{\partial^2 f}{\partial x_i \partial x_j} \quad\overset{i=j}{\rightarrow}\quad 
	\frac{\partial^2 f}{\partial x_i^2} \] 
	If the curvature is zero, then the slope does not change and hence the function is a straight line (1D), a plane (2D) or a hyperplane ($n$D).
	\begin{itemize}
		\item \textbf{Negative} second derivative means, that the slope is getting smaller as the argument gets bigger and hence the function is curving down
		\begin{itemize}
			\item[$\rightarrow$] Hence using the gradient alone, the function value will be overestimated, i.e., the function decreases more than predicted by using the gradient alone
		\end{itemize}
		\item \textbf{Zero} second derivative means, that the slope is constant as the argument gets bigger and hence the function has no curvature
		\begin{itemize}
			\item[$\rightarrow$] Hence using the gradient alone, the function value will be perfectly estimated, i.e., the function decreases as predicted by using the gradient alone
			\item[$\rightarrow$] I.e., for a slope of 1, the function will increase by $\epsilon$ when the argument is increased by $\epsilon$
		\end{itemize}
		\item \textbf{Positive} second derivative means, that the slope is getting larger as the argument gets bigger and hence the function is curving up
		\begin{itemize}
			\item[$\rightarrow$] Hence using the gradient alone, the function value will be underestimated, i.e., the function decreases less than predicted by using the gradient alone
		\end{itemize}
	\end{itemize}
	
	If the scalar function $f(\bol{x})$ has vector argument $\bol{x}$, then there are many second derivatives, which are collected together in the \textbf{Hessian}:
	\[ \bol{H}(f)(\bol{x})_{i,j} = \frac{\partial^2 f(\bol{x})}{\partial x_i\partial x_j} 
	\quad\rightarrow\quad H_{i,j}=H_{j,i} \]
	Note that the Hessian is nothing else than the Jacobian of the gradient of $f()$.
	Also, the Hessians are \textbf{real and symmetric}, which means they can be decomposed into a set of real eigenvalues and orthogonal eigenvectors.\\
	
	As in the one-dimensional case, the directional second order derivative allows for a prediction how well a gradient descent step will perform. A \textbf{second order Taylor series approximation} of the function $f(\bol{x})$ around a point $\bol{x}^{(0)}$ is given by the formula
	\[ f(\bol{x}) \approx f(\bol{x}^{(0)}) + \left( \bol{x}-\bol{x}^{(0)} \right)^T\bol{g}+
	\frac{1}{2} \left( \bol{x}-\bol{x}^{(0)} \right)^T\bol{H}\left( \bol{x}-\bol{x}^{(0)} \right) \]
	
	Given we are at $\bol{x}^{(0)}$ and now we take a small step $\epsilon$ into the direction of the negative gradient $\bol{g}$, then the new point $\bol{x}$ will be given by $\bol{x}=\bol{x}^{(0)}-\epsilon\bol{g}$.
	Now the second order Taylor approximation can be evaluated at this new point $\bol{x}=\bol{x}^{(0)}-\epsilon\bol{g}$:
	\[ f(\bol{x}^{(0)}-\epsilon\bol{g}) \approx f(\bol{x}^{(0)})-\epsilon \bol{g}^T\bol{g}+\frac{1}{2}
	\epsilon^2 \bol{g}^T\bol{H}\bol{g} \] 
	
	\textbf{Newtons method:}
	\begin{align*}
	f(\bol{x}) &\approx f(\bol{x}^{(0)}) + \left( \bol{x}-\bol{x}^{(0)} \right)^T \nabla_x f(\bol{x}^{(0)}) \\
	&+\frac{1}{2} \left( \bol{x}-\bol{x}^{(0)} \right)^T \bol{H}(f)(\bol{x}^{(0)})\left( \bol{x}-\bol{x}^{(0)} \right)
	\end{align*}
	We can now analytically find the minimum of this quadratic function:
	\[ \bol{x}^\ast = \bol{x}^{(0)} - \bol{H}(f)(\bol{x}^{(0)})^{-1}\nabla_x f(\bol{x}^{(0)}) \]
	
	Problems with Newtons method in deep learning:
	\begin{itemize}
		\item In high dimensions, there are many more saddle points than local minima
		\item Unfortunately Newtonâ€™s method will jump to a saddle point and then will be stuck
		\item This is not true for gradient descent
	\end{itemize}
	
	\subsection{Example: Linear Least Squares}
	In linear least squares, the goal is to find the vector $\bol{x}$ that minimizes the expression
	\[ f(\bol{x}) = \frac{1}{2} \lVert \bol{Ax}-\bol{b}\rVert_2^2 \]
	There are many algorithms to do this, but we will use gradient descent.
	Hence the gradient of this function needs to be calculated:
	\[ \nabla_xf(\bol{x}) =\bol{A}^T (\bol{Ax}-\bol{b}) = \bol{A}^T\bol{Ax}-\bol{A}^T\bol{b} \]
	Now we can follow the negative gradient in little steps to find the minimum:

	\begin{align*}
	\bol{x} &= \bol{x}-\epsilon \nabla_x f(\bol{x})\\
	&= \bol{x} -\epsilon \left(\bol{A}^T\bolA\bol{x}-\bolA^T\bol{b}\right)
	\end{align*}
	These steps are taken as long as the $L^2$ norm of the gradient does not fall below some minimum $\delta$:
	\[ \lVert \bolA^T\bol{Ax}-\bolA^T\bol{b}\rVert_2 > \delta \]
	
	
\end{multicols}
\newpage






























